2. 어텐션 기반 다변량 예측 모형

어텐션(Attention) : 입력 시퀀스의 각 시점에 대해 다른 시점들과의 관련성을 가중치로 계산하여, 예측에 중요한 정보를 선택적으로 활용. 
◦ Query(쿼리), Key(키), Value(값) 구조를 통해 각 시점이 다른 시점들과 얼마나 관련있는지 유사도 점수를 계산하고, 이를 정규화하여 가중합으로 변환.
◦ 모델은 장기 의존성(long-term dependency)을 학습할 수 있고, 변수 간의 복잡한 관계를 포착할 수 있음.

□ TFT(Temporal Fusion Transformer)
◦ 각 시점(time step)에서 변수들을 선택하고 임베딩하여 [B, T, d] 형태의 텐서로 변환: X IN  R ^{B TIMES  T TIMES  N} rarrow  E IN  R ^{B TIMES  T TIMES  d}
◦ 각 시점에서 예측에 중요한 변수를 선택한 후, 선택된 변수들을 MLP로 임베딩
◦ LSTM을 통해 시간 의존성을 먼저 학습한 후, 어텐션은 시간축(time dimension) 방향으로 적용되어 시점 간의 의존성을 학습
◦ B는 배치 크기, T는 샘플 내 시점 수, N은 변수의 개수, d는 임베딩 차원을 의미
◦ 트랜스포머 구조를 적용한 초창기 딥러닝 모델로 알려져 있음

□ PatchTST(Patch Time Series Transformer)
◦ 각 변수의 입력 시퀸스(T 길이) 시계열을 패치(patch)로 분할하여 각 패치를 하나의 토큰으로 임베딩하여 [B, N, P, d] 형태의 텐서로 변환:
◦ 시계열을 연속된 시점들의 패치로 분할한 후, 각 패치를 독립적으로 임베딩:  X IN  R ^{B TIMES  N TIMES  T} rarrow  P IN  R ^{B TIMES  N TIMES  P TIMES  p} rarrow  E IN  R ^{BN TIMES  P TIMES  d}
◦ Channel Independence로 각 변수별로 독립적으로 처리하며, 어텐션은 시간축(패치 간) 방향으로 적용되어 패치 간 의존성을 학습
◦ B는 배치 크기, T는 샘플 내 시점 수, N은 변수의 개수, P는 패치 개수, p는 패치 길이, d는 임베딩 차원을 의미
◦ 패치 단위 임베딩을 통해 계산 효율성을 높이고 장기 의존성을 효과적으로 포착

□ iTransformer(Inverted Transformer)
◦ 각 변수의 입력 시퀸스(T 길이)를 하나의 토큰으로 변환: PatchTST가 변수 간 의존성을 효과적으로 학습하지 못하는 부분 보완
◦ 시계열을 독립된 임베딩 벡터로 압축하여 변수간 상관관계를 모델링 할 수 있도록 임베딩: X IN  R ^{B TIMES  T TIMES  N} rarrow  X PRIME  IN  R ^{B TIMES  N TIMES  T} rarrow  E IN  R ^{B TIMES  N TIMES  d}
◦ 어텐션은 시간축이 아닌 변수축(variate dimension) 방향으로 적용되어 변수 간 상관관계를 직접 모델링하며, 시간 의존성은 변수별로 독립적인 Linear(MLP)로 압축하여 처리
◦ B는 배치 크기, T는 샘플 내 시점 수, N은 변수의 개수, d는 임베딩 차원을 의미
◦ 기존 Transformer 구조를 역전시켜 변수 간 상호작용을 명시적으로 학습

□ 단기예측 실험 구성
◦ 2019년 12월 4주까지의 데이터로 모델 학습 후  2024년 1월 1주 ~ 2025년 9월 4주를 한 주 단위로 재귀적 예측
◦ 어텐션 계열 모델의 경우 결측치가 있으면 학습이 불가능하기 때문에, 보간을 적용
◦ 학습은 ‘주간’ 데이터를 기준으로 이루어진 반면 예측은 ‘월간’ 데이터 이므로 주간 예측을 실시하고 월별로 집계:  [0.1, 0.2, 0.3, 0.4] 가중치 활용
◦ 전체 sMSE(standardized mean squared error), sMAE(standardized mean absolute error)값(총 21개)의 평균으로 성능 평가
◦ 매 시점(time step)마다 최신 데이터를 반영(전체 가중치 재학습은 시행 X), 예측 모델 업데이트 및 예측 실행
◦ 어텐션 계열 모델은 별도의 변환이나 정규화를 실시하지 않고 모델 내부에서 처리

□ 단기예측 실험 결과

| 모델명 | 생산 지표 단기 예측 | | 투자 지표 단기 예측 | |
|--------|-------------------|--|-------------------|--|
| | sMSE | sMAE | sMSE | sMAE |
| TFT | 0.56 | 0.66 | 2.23 | 1.11 |
| PatchTST | 0.68 | 0.72 | 2.02 | 1.05 |
| iTransformer | 0.12 | 0.28 | 1.09 | 0.77 |

(plot here)[combined_attention_forecast.png]

□ 단기예측 결과 해석
◦ 시계열 자체를 토큰화 하는 iTransformer의 경우 예측치의 분산이 낮게 나타나 양호한 평가 지표를 보임
◦ TFT, PatchTST의 경우 재귀적 예측에서 예측치의 분산이 상대적으로 크지만 그만큼 단기적 변동을 잘 잡아낼 수 있음

□ 장기예측 실험 구성
◦ 2024년 11월 첫째 주를 기준으로 4주, 8주, 12주, ..., 40주(최대 10개월) 후 예측을 수행하여 예측 기간에 따른 모델 성능 변화를 비교
◦ 하나의 모델을 최대 예측 기간(40주)에 맞춰 학습한 후, 동일한 모델을 사용하여 다양한 horizon(4주, 8주, 12주, ..., 40주)에 대한 예측을 수행
◦ 단기 예측 실험과 마찬가지로 월별 데이터를 기준으로 평가 지표 계산
◦ 재귀적 예측과 달리 최신 데이터를 반영하는 모델 업데이트는 처음 한번만 실행

□ 장기예측 실험 결과

